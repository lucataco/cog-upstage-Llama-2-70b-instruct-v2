#!/usr/bin/env python

import os
import sys
from transformers import AutoTokenizer
from auto_gptq import AutoGPTQForCausalLM

# append project directory to path so predict.py can be imported
sys.path.append('.')

from predict import MODEL_NAME, MODEL_BASENAME, CACHE
use_triton = True

# tokenizer = AutoTokenizer.from_pretrained(
#     MODEL_NAME,
#     use_fast=True,
# )
# tokenizer.save_pretrained(CACHE, safe_serialization=True)

# model = AutoGPTQForCausalLM.from_quantized(
#     MODEL_NAME,
#     model_basename=MODEL_BASENAME,
#     use_safetensors=True,
#     trust_remote_code=True,
#     use_triton=use_triton,
# )
# model.save_quantized(CACHE, use_safetensors=True)

# Manually run:
os.system("git clone --branch gptq-4bit-32g-actorder_True https://huggingface.co/TheBloke/Upstage-Llama-2-70B-instruct-v2-GPTQ cache")
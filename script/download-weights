#!/usr/bin/env python

import os
import sys
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# append project directory to path so predict.py can be imported
sys.path.append('.')

from predict import MODEL_NAME, MODEL_CACHE, TOKEN_CACHE

# Make cache folders
if not os.path.exists(MODEL_CACHE):
    os.makedirs(MODEL_CACHE)

if not os.path.exists(TOKEN_CACHE):
    os.makedirs(TOKEN_CACHE)

tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    cache_dir=TOKEN_CACHE,
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    load_in_8bit=True,
    rope_scaling={"type": "dynamic", "factor": 2},
    cache_dir=MODEL_CACHE,
)
